{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'librosa' from '/home/arjunp/.conda/envs/codebase3/lib/python3.8/site-packages/librosa/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "import math  \n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "import jiwer\n",
    "import torch\n",
    "import accelerate\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "!module load openmind/ffmpeg/20160310 \n",
    "from importlib import reload\n",
    "import librosa\n",
    "import transformers\n",
    "reload(transformers)\n",
    "reload(transformers.models)\n",
    "\n",
    "reload(librosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 28 16:44:38 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:3B:00.0 Off |                  N/A |\n",
      "| 32%   40C    P8             13W /  250W |    4051MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3217699      C   ...np/.conda/envs/codebase3/bin/python       2856MiB |\n",
      "|    0   N/A  N/A   3223613      C   ...np/.conda/envs/codebase3/bin/python       1192MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arjunp/.conda/envs/codebase3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = glob.glob(\"/om/user/arjunp/smallFineTune/data/*/*\")\n",
    "text = ['hello', 'how are you']\n",
    "for i in list_of_files:\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    csv_path = \"/om/user/arjunp/fakeTranscripts/\"+os.path.basename(i)[:-4]+\".csv\"\n",
    "    df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdir = '/om/user/arjunp/smallFineTune'\n",
    "datadir = '/om/user/arjunp/smallFineTune/data'\n",
    "golddir = '/om/user/arjunp/fakeTranscripts'\n",
    "transcriptlist = []\n",
    "filepathlist = []\n",
    "\n",
    "for split in os.listdir(datadir):\n",
    "    split_path = os.path.join(datadir, split)\n",
    "    for file_path in os.listdir(split_path):\n",
    "        name = os.path.join(\"data\", os.path.basename(split_path),os.path.basename(file_path))\n",
    "        goldpath = file_path[:-4]+\".csv\"\n",
    "        df = pd.read_csv(os.path.join(golddir,goldpath))\n",
    "        transcriptlist.append(df.text.str.cat(sep=' '))\n",
    "        filepathlist.append(name)\n",
    "\n",
    "transcriptdf = pd.DataFrame(list(zip(filepathlist, transcriptlist)), columns =['file_name', 'transcription'])\n",
    "transcriptdf.to_csv('/om/user/arjunp/smallFineTune/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a2b6e491b4df5945360b29ec1a955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84f52461a7d49d3ad75c31bd889bb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"/om/user/arjunp/fineTuningDataFolders/smallFineTune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/weka/scratch/weka/cpl/arjunp/smallFineTune/data/test/1029_xxxx_139930.wav',\n",
       "  'array': array([-0.04031372,  0.03128052,  0.040802  , ..., -0.0015564 ,\n",
       "         -0.00521851, -0.00189209]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'hello how are you'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small.en\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small.en\",   task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 hello how are you\n",
      "Decoded w/ special:    <|startoftranscript|><|transcribe|><|notimestamps|>hello how are you<|endoftext|>\n",
      "Decoded w/out special: hello how are you\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\",  task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeldict = {\"input_ids\": labels}\n",
    "teens = processor.tokenizer.pad(labeldict, return_tensors=\"pt\")\n",
    "teens['input_ids'].shape\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b97639114a04568bc3498c34ffee6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07dfe60e5f14b288e383a8649fbb992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    #max_length=1024,padding='max_length'\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0] #log-Mel input features\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset  = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small.en\")\n",
    "model.generation_config.language = \"english\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "#model.config.max_length = 2048\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False\n",
    "#model.config.max_target_positions = 2048\n",
    "#model.config.max_source_positions = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperConfig {\n",
       "  \"_name_or_path\": \"openai/whisper-small.en\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"apply_spec_augment\": false,\n",
       "  \"architectures\": [\n",
       "    \"WhisperForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"begin_suppress_tokens\": [\n",
       "    220,\n",
       "    50256\n",
       "  ],\n",
       "  \"bos_token_id\": 50257,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 50257,\n",
       "  \"dropout\": 0.0,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"forced_decoder_ids\": null,\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_prob\": 0.05,\n",
       "  \"max_length\": 448,\n",
       "  \"max_source_positions\": 1500,\n",
       "  \"max_target_positions\": 1024,\n",
       "  \"median_filter_width\": 7,\n",
       "  \"model_type\": \"whisper\",\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_mel_bins\": 80,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"scale_embedding\": false,\n",
       "  \"suppress_tokens\": [],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_cache\": false,\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 51864\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "a = np.array(dataset['train'][0]['input_features']) \n",
    "b = resample(a, 2048, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 2048)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "          \n",
    "        #audio input features to torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        \n",
    "        #resampling attempt below\n",
    "        '''\n",
    "            for i in range(len(input_features)):\n",
    "                a = np.array(input_features[i]['input_features']) \n",
    "                b = resample(a, 2048, axis=1)\n",
    "                input_features[i]['input_features'] = b\n",
    "        '''\n",
    "\n",
    "        #print(np.array(input_features[0]['input_features']).shape)\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\" , padding=True )\n",
    "\n",
    "        #tokenized label sequences and padding\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\" , padding=True)\n",
    "         \n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        #for i in batch[\"labels\"]:\n",
    "            #print(len(i))\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor,decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/arjunp/.conda/envs/codebase3/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/10 00:00 < 00:05, 1.23 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50256]}\n"
     ]
    }
   ],
   "source": [
    "## For the actual training loop:\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/om/user/arjunp/whisper-small-finetuned-dummy\",  \n",
    "    per_device_train_batch_size=6, #changing this \n",
    "    gradient_accumulation_steps=1,   \n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    #max_steps=5000,\n",
    "    num_train_epochs = 10, #added new\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    #generation_max_length=1024, #changed to 700\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to= \"none\",  #changed to none from wandb\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False, #changed to false\n",
    "    remove_unused_columns=False #added this new arg\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "     \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('./whisper-small-finetuned-dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = WhisperForConditionalGeneration.from_pretrained(\"/om/user/arjunp/whisper-small-finetuned-dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "model2.to(device)\n",
    "original_model_weights = model.state_dict() \n",
    "new_model_weights = model2.state_dict() \n",
    "for layer in original_model_weights:\n",
    "    if  not torch.equal(original_model_weights[layer], new_model_weights[layer]):\n",
    "        print(f\"Layer {layer} has changed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c3kernel",
   "language": "python",
   "name": "codebase3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
